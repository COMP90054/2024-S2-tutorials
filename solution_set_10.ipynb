{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/COMP90054/2024-S2-tutorials/blob/main/solution_set_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wtYhEJOLntKg"
      },
      "source": [
        "# COMP90054 AI Planning for Autonomy\n",
        "### Solution Set 10: Policy iteration and reward shaping\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCLHR5J_0mue"
      },
      "source": [
        "### Problem 1: Policy update\n",
        "\n",
        "Policy Iteration has two main steps, policy evaluation and policy update. In order to evaluate the given policy:\n",
        "\n",
        "$\\begin{array}{lll}\n",
        "V^{\\pi}(Messi) & = & Q^{\\pi}(Messi, Pass)\\\\\n",
        "               & = & P_{Pass}(Suarez \\mid Messi)[r(Messi,pass,Suarez) + \\gamma \\cdot V^{\\pi}(Suarez)]\\\\\n",
        "               & = &\\gamma \\cdot V^{\\pi}(Suarez) −1\\\\[2mm]\n",
        "V^{\\pi}(Suarez) & = & Q^{\\pi}(Suraez, Pass)\\\\\n",
        "               & = & P_{Pass}(Messi \\mid Suarez)[r(Suarez,pass,Messi) + \\gamma \\cdot V^{\\pi}(Messi)]\\\\     \n",
        "               & = & \\gamma \\cdot V^{\\pi}(Messi) −1 \\\\[2mm]\n",
        "V^{\\pi}(Scored) & = & Q^{\\pi}(Scored, Return)\\\\\n",
        "               & = & P_{Return}(Messi \\mid Scored)[r(Scored,pass,Messi) + \\gamma \\cdot V^{\\pi}(Messi)]\\\\\n",
        "               & = & \\gamma \\cdot V^{\\pi}(Messi) + 2\n",
        "\\end{array}$\n",
        "\n",
        "Then, we solve a basic simultaneous linear equation (not part of the subject learning outcomes) about $V^{\\pi}(Messi)$ and $V^{\\pi}(Suarez)$:\n",
        "\n",
        "$\\begin{array}{lll}\n",
        "V^{\\pi}(Messi) & = & 1/(\\gamma -1)\\\\\n",
        "V^{\\pi}(Suarez) & = & 1/(\\gamma -1)\\\\\n",
        "V^{\\pi}(Scored) & = & 3 + 1/(\\gamma -1)\n",
        "\\end{array}$\n",
        "\n",
        "Then apply $\\gamma = 0.8), the policy evaluation table would be:\n",
        "\n",
        "Iteration  | Q(Messi, P) | Q(Messi, S)  | Q(Suarez, P)  | Q(Suarez, S) | Q(Scored)\n",
        "-----------|---|----|----|--------------|-------\n",
        "0  |0.0|0.0|0.0|0.0|0.0|\n",
        "1  | -5| -5.52| -5 | -4.56 | -2|\n",
        "2  |-4.194| -4.772|-4.355|-3.993|-1.355\n",
        "\n",
        "Then we apply  two iterations of policy update based on the above table to get:\n",
        "\n",
        "Iteration  | $\\pi$(Messi) | $\\pi$(Suarez)  | $\\pi$(Scored)\n",
        "-----------|---|----|----\n",
        "0  |Pass|Pass|Return\n",
        "1  |Pass | Shoot| Return\n",
        "2  |Pass | Shoot |Return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAVg7eLm1K8M"
      },
      "source": [
        "### Problem 2: Potential functions\n",
        "\n",
        "The important thing for the reward function is that you need to consider the next goal and\n",
        "whether the key is held. Using normalised Manhattan distance as the estimate, we can define the\n",
        "following potential function:\n",
        "\n",
        "```\n",
        "if Key == 0:\n",
        "    return 1 - NormalizedManhattan(s, K)\n",
        "else if Key == 1 and M == False:\n",
        "    return 1 - NormalizedManhattan(s, M)\n",
        "else if Key == 1 and M == True:\n",
        "    return 1 - NormalizedManhattan(s, R)\n",
        "else if Key == 2:\n",
        "    return 1 - NormalizedManhattan(s, R)\n",
        "```\n",
        "\n",
        "Others are possible, but this one will help to guide the agent early in the exploration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLm4V9UO1Oot"
      },
      "source": [
        "### Problem 3: Reward shaping update\n",
        "\n",
        "Assuming that the Manhattan function is normalise, we calculate $\\Phi$ for the following:\n",
        "\n",
        "Let $s$ be the current state, $s_1$ be the state after action Up and $s_2$ be the state after action Right:\n",
        "\n",
        "$\n",
        "\\begin{array}{lll}\n",
        " s & = & ((4,0), Key = 1,M = False)\\\\\n",
        "s_1 & = & ((4,1),Key = 1,M = False)\\\\\n",
        "s_2 & = & ((5,0),Key = 1,M = False)\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "Then the $\\Phi$ value of each state is:\n",
        "$\n",
        "\\begin{array}{lll}\n",
        "\\Phi(s) & = & 1 − \\frac{9}{12} & = & \\frac{3}{12}\\\\\n",
        "\\Phi(s_1) & = & 1 − \\frac{8}{12} & = & \\frac{4}{12}\\\\\n",
        "\\Phi(s_2) & = & 1 − \\frac{10}{12} & = & \\frac{2}{12}\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "To update the Up action:\n",
        "\n",
        "$\n",
        "\\begin{array}{lll}\n",
        "Q(s, Up) & \\rightarrow & Q(s, Up) + \\alpha[r(s,Up,s_1) + F(s,s_1) + \\gamma \\max Q(s_1,a') - Q(s,Up)]\\\\\n",
        "        & \\rightarrow  & 0 + 0.2 \\times [0 + 0.9 \\times \\frac{4}{12} − \\frac{3}{12} + 0.9 \\times 0 - 0]\\\\\n",
        "        & \\rightarrow & 0.01\n",
        "\\end{array}\n",
        "$\n",
        "\n",
        "To update the Right action:\n",
        "\n",
        "$\n",
        "\\begin{array}{lll}\n",
        "Q(s, Right) & \\rightarrow & Q(s, Right) + \\alpha[r(s,Right,s_2) + F(s,s_2) + \\gamma \\max Q(s_2,a') - Q(s,Right)]\\\\\n",
        "        & \\rightarrow  & 0 + 0.2 \\times [0 + 0.9 \\times \\frac{2}{12} − \\frac{3}{12} + 0.9 \\times 0 - 0]\\\\\n",
        "        & \\rightarrow & -0.02\n",
        "\\end{array}\n",
        "$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "HJLiN8UdxEF-"
      ],
      "name": "solution_set_10.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}